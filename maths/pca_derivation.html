<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Principal Component Analysis - Complete Mathematical Derivation</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
            displayAlign: "left"
        });
    </script>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        .container {
            background: white;
            border-radius: 15px;
            padding: 40px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
            text-align: center;
            font-size: 2.5em;
        }
        h2 {
            color: #34495e;
            margin-top: 30px;
            border-left: 4px solid #764ba2;
            padding-left: 10px;
        }
        h3 {
            color: #7f8c8d;
            margin-top: 20px;
        }
        .theorem-box {
            background: #f8f9fa;
            border: 2px solid #667eea;
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
        }
        .proof-box {
            background: #fff3cd;
            border-left: 5px solid #ffc107;
            padding: 15px;
            margin: 15px 0;
        }
        .key-insight {
            background: #d4edda;
            border: 1px solid #28a745;
            border-radius: 8px;
            padding: 15px;
            margin: 20px 0;
        }
        .equation {
            background: #f0f8ff;
            padding: 15px;
            margin: 15px 0;
            border-radius: 5px;
            overflow-x: auto;
        }
        .properties {
            background: #e8f4fd;
            border-radius: 8px;
            padding: 15px;
            margin: 15px 0;
        }
        ul {
            line-height: 1.8;
        }
        li {
            margin: 10px 0;
        }
        code {
            background: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }
        .note {
            background: #fef3c7;
            border: 1px solid #f59e0b;
            border-radius: 6px;
            padding: 12px;
            margin: 15px 0;
        }
        .algorithm-box {
            background: #f3e5ff;
            border: 2px solid #8b5cf6;
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
        }
        .final-summary {
            background: linear-gradient(135deg, #667eea20, #764ba220);
            border: 2px solid #667eea;
            border-radius: 10px;
            padding: 25px;
            margin-top: 30px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Principal Component Analysis (PCA)</h1>
        <h3 style="text-align: center; color: #667eea;">Complete Mathematical Derivation for Face Recognition</h3>

        <h2>1. Problem Formulation</h2>
        <div class="theorem-box">
            <p>Given a dataset with <strong>n</strong> samples: $\mathbf{X} = \{\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_n\}$ where $\mathbf{x}_i \in \mathbb{R}^d$</p>
            
            <h3>Step 1: Center the data</h3>
            <div class="equation">
                $$\bar{\mathbf{x}} = \frac{1}{n} \sum_{i=1}^{n} \mathbf{x}_i \quad \text{(mean vector)}$$
                $$\mathbf{X}_{centered} = \{\mathbf{x}_i - \bar{\mathbf{x}}\} \quad \text{for all } i$$
            </div>
            
            <h3>Step 2: Construct the data matrix</h3>
            <div class="equation">
                $$\mathbf{X} = [\mathbf{x}_1 - \bar{\mathbf{x}}, \mathbf{x}_2 - \bar{\mathbf{x}}, ..., \mathbf{x}_n - \bar{\mathbf{x}}]^T \in \mathbb{R}^{n \times d}$$
            </div>
        </div>

        <h2>2. Covariance Matrix</h2>
        <div class="theorem-box">
            <p>The sample covariance matrix <strong>C</strong> is defined as:</p>
            <div class="equation">
                $$\mathbf{C} = \frac{1}{n-1} \mathbf{X}^T\mathbf{X} = \frac{1}{n-1} \sum_{i=1}^{n} (\mathbf{x}_i - \bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})^T$$
            </div>
            
            <div class="properties">
                <h3>Properties of C:</h3>
                <ul>
                    <li><strong>Symmetric:</strong> $\mathbf{C} = \mathbf{C}^T$</li>
                    <li><strong>Positive semi-definite:</strong> $\mathbf{v}^T\mathbf{C}\mathbf{v} \geq 0$ for all $\mathbf{v} \in \mathbb{R}^d$</li>
                    <li><strong>Dimensions:</strong> $\mathbf{C} \in \mathbb{R}^{d \times d}$</li>
                </ul>
            </div>
        </div>

        <h2>3. Finding the First Principal Component</h2>
        <div class="algorithm-box">
            <h3>Objective</h3>
            <p>Find unit vector $\mathbf{w}_1$ that maximizes the variance of projected data.</p>
            
            <h3>Variance along direction w:</h3>
            <div class="equation">
                $$\text{Var}(\mathbf{X}\mathbf{w}) = E[(\mathbf{X}\mathbf{w})^T(\mathbf{X}\mathbf{w})] = \mathbf{w}^T E[\mathbf{X}^T\mathbf{X}]\mathbf{w} = \mathbf{w}^T\mathbf{C}\mathbf{w}$$
            </div>
            
            <h3>Optimization problem:</h3>
            <div class="equation">
                $$\begin{align}
                \text{maximize} \quad & f(\mathbf{w}) = \mathbf{w}^T\mathbf{C}\mathbf{w} \\
                \text{subject to} \quad & ||\mathbf{w}||^2 = \mathbf{w}^T\mathbf{w} = 1
                \end{align}$$
            </div>
        </div>

        <h2>4. Lagrangian Method</h2>
        <div class="proof-box">
            <p>Introduce Lagrange multiplier $\lambda$:</p>
            <div class="equation">
                $$L(\mathbf{w}, \lambda) = \mathbf{w}^T\mathbf{C}\mathbf{w} - \lambda(\mathbf{w}^T\mathbf{w} - 1)$$
            </div>
            
            <p><strong>Take derivative with respect to w:</strong></p>
            <div class="equation">
                $$\frac{\partial L}{\partial \mathbf{w}} = 2\mathbf{C}\mathbf{w} - 2\lambda\mathbf{w} = 0$$
            </div>
            
            <p><strong>This yields the eigenvalue equation:</strong></p>
            <div class="equation" style="background: #ffe4e1; border: 2px solid #ff6b6b;">
                $$\mathbf{C}\mathbf{w} = \lambda\mathbf{w}$$
            </div>
            
            <div class="key-insight">
                <strong>Key Result:</strong> The first principal component $\mathbf{w}_1$ is the eigenvector corresponding to the largest eigenvalue $\lambda_1$ of $\mathbf{C}$.
            </div>
        </div>

        <h2>5. Proof that λ Represents Variance</h2>
        <div class="proof-box">
            <p>When $\mathbf{w}$ is an eigenvector with eigenvalue $\lambda$:</p>
            <div class="equation">
                $$\text{Variance} = \mathbf{w}^T\mathbf{C}\mathbf{w} = \mathbf{w}^T(\lambda\mathbf{w}) = \lambda(\mathbf{w}^T\mathbf{w}) = \lambda$$
            </div>
            <p>Since $||\mathbf{w}|| = 1$, the eigenvalue $\lambda$ directly equals the variance along that direction.</p>
        </div>

        <h2>6. Finding Subsequent Principal Components</h2>
        <div class="algorithm-box">
            <h3>For the k-th principal component $\mathbf{w}_k$:</h3>
            
            <p><strong>Optimization problem:</strong></p>
            <div class="equation">
                $$\begin{align}
                \text{maximize} \quad & \mathbf{w}_k^T\mathbf{C}\mathbf{w}_k \\
                \text{subject to} \quad & ||\mathbf{w}_k||^2 = 1 \\
                & \mathbf{w}_k^T\mathbf{w}_j = 0 \quad \text{for } j = 1, 2, ..., k-1
                \end{align}$$
            </div>
            
            <p><strong>Lagrangian with multiple constraints:</strong></p>
            <div class="equation">
                $$L = \mathbf{w}_k^T\mathbf{C}\mathbf{w}_k - \lambda_k(\mathbf{w}_k^T\mathbf{w}_k - 1) - \sum_{j=1}^{k-1} \mu_j(\mathbf{w}_k^T\mathbf{w}_j)$$
            </div>
            
            <div class="note">
                <p><strong>Result:</strong> Through orthogonality constraints, this simplifies back to: $\mathbf{C}\mathbf{w}_k = \lambda_k\mathbf{w}_k$</p>
            </div>
        </div>

        <h2>7. Complete Solution via Eigendecomposition</h2>
        <div class="theorem-box">
            <p>Since $\mathbf{C}$ is symmetric, it can be diagonalized:</p>
            <div class="equation">
                $$\mathbf{C} = \mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^T$$
            </div>
            
            <p>Where:</p>
            <ul>
                <li>$\mathbf{Q} = [\mathbf{w}_1, \mathbf{w}_2, ..., \mathbf{w}_d]$ is orthogonal matrix of eigenvectors</li>
                <li>$\mathbf{\Lambda} = \text{diag}(\lambda_1, \lambda_2, ..., \lambda_d)$ with $\lambda_1 \geq \lambda_2 \geq ... \geq \lambda_d \geq 0$</li>
            </ul>
        </div>

        <h2>8. Dimensionality Reduction</h2>
        <div class="algorithm-box">
            <h3>Project data onto k principal components:</h3>
            <div class="equation">
                $$\mathbf{Y} = \mathbf{X}\mathbf{W}_k$$
            </div>
            <p>Where $\mathbf{W}_k = [\mathbf{w}_1, \mathbf{w}_2, ..., \mathbf{w}_k] \in \mathbb{R}^{d \times k}$</p>
            
            <h3>Reconstruction:</h3>
            <div class="equation">
                $$\hat{\mathbf{X}} = \mathbf{Y}\mathbf{W}_k^T = \mathbf{X}(\mathbf{W}_k\mathbf{W}_k^T)$$
            </div>
            
            <h3>Reconstruction error:</h3>
            <div class="equation">
                $$||\mathbf{X} - \hat{\mathbf{X}}||_F^2 = \sum_{i=k+1}^{d} \lambda_i$$
            </div>
        </div>

        <h2>9. Total Variance Explained</h2>
        <div class="properties">
            <h3>Total variance in original space:</h3>
            <div class="equation">
                $$\text{Total Var} = \text{trace}(\mathbf{C}) = \sum_{i=1}^{d} \lambda_i$$
            </div>
            
            <h3>Variance explained by first k components:</h3>
            <div class="equation">
                $$\text{Explained Var} = \sum_{i=1}^{k} \lambda_i$$
            </div>
            
            <h3>Proportion of variance explained:</h3>
            <div class="equation" style="background: #e7f3ff; border: 2px solid #0066cc;">
                $$\text{PVE} = \frac{\sum_{i=1}^{k} \lambda_i}{\sum_{i=1}^{d} \lambda_i}$$
            </div>
        </div>

        <h2>10. Connection to SVD</h2>
        <div class="theorem-box">
            <p>For centered data matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$:</p>
            <div class="equation">
                $$\mathbf{X} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T \quad \text{(SVD decomposition)}$$
            </div>
            
            <p>Then:</p>
            <div class="equation">
                $$\mathbf{C} = \frac{1}{n-1}\mathbf{X}^T\mathbf{X} = \frac{1}{n-1}\mathbf{V}\mathbf{\Sigma}^2\mathbf{V}^T$$
            </div>
            
            <div class="key-insight">
                <p>Therefore:</p>
                <ul>
                    <li>Principal components = columns of $\mathbf{V}$</li>
                    <li>Eigenvalues $\lambda_i = \sigma_i^2/(n-1)$ where $\sigma_i$ are singular values</li>
                </ul>
            </div>
        </div>

        <h2>11. Application to Face Recognition</h2>
        <div class="algorithm-box">
            <h3>Eigenfaces Method</h3>
            <ol>
                <li><strong>Vectorize faces:</strong> Convert each $m \times n$ face image to vector $\mathbf{x}_i \in \mathbb{R}^{mn}$</li>
                <li><strong>Compute mean face:</strong> $\bar{\mathbf{x}} = \frac{1}{N}\sum_{i=1}^{N}\mathbf{x}_i$</li>
                <li><strong>Center the data:</strong> $\mathbf{X}_{centered} = \mathbf{X} - \bar{\mathbf{x}}\mathbf{1}^T$</li>
                <li><strong>Compute eigenfaces:</strong> Principal components of face covariance matrix</li>
                <li><strong>Project faces:</strong> $\mathbf{y}_i = \mathbf{W}_k^T(\mathbf{x}_i - \bar{\mathbf{x}})$</li>
                <li><strong>Recognition:</strong> Compare projections using distance metrics</li>
            </ol>
        </div>

        <div class="final-summary">
            <h2>Key Insights</h2>
            <ol>
                <li><strong>PCA finds orthogonal directions</strong> that maximize variance sequentially</li>
                <li><strong>Eigenvalues</strong> quantify variance along each principal component</li>
                <li><strong>Eigenvectors</strong> define the transformation to the new coordinate system</li>
                <li><strong>Optimal k-dimensional approximation</strong> in terms of mean squared error</li>
                <li><strong>Computational efficiency</strong> through SVD avoids explicit covariance matrix computation</li>
                <li><strong>For face recognition:</strong> Eigenfaces capture the most significant variations in facial structure</li>
            </ol>
        </div>

        <div class="note" style="margin-top: 30px;">
            <p><strong>Implementation Note:</strong> For large-scale face recognition (n ≫ d), use the "kernel trick" where we compute eigenvectors of $\mathbf{X}\mathbf{X}^T$ (size $n \times n$) instead of $\mathbf{X}^T\mathbf{X}$ (size $d \times d$), then transform back to get eigenfaces.</p>
        </div>
    </div>
</body>
</html>